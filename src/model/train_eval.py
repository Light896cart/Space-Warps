from typing import Optional, Dict, Any

import torch.optim as optim
import torch
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import DataLoader

def train_and_evaluate_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int = 10,
    lr: float = 1e-3,
    device: str = None,
    save_best_path: str = "best_model.pth",
    verbose: bool = True,
    seed: int = 42,
    weight_model: Optional[torch.Tensor] = None,
) -> Dict[str, Any]:
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –µ—ë –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –ø–æ accuracy –º–æ–¥–µ–ª—å.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ (warm-start).

    Args:
        model: PyTorch –º–æ–¥–µ–ª—å (—É–∂–µ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —Å–ª–æ–∏)
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        lr: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        device: 'cuda' –∏–ª–∏ 'cpu'. –ï—Å–ª–∏ None ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
        save_best_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        verbose: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –ª–æ–≥–∏
        seed: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –æ–±—É—á–µ–Ω–∏—è

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - best_val_acc: –ª—É—á—à–∞—è accuracy –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            - train_losses: loss –ø–æ —ç–ø–æ—Ö–∞–º
            - val_accuracies: accuracy –ø–æ —ç–ø–æ—Ö–∞–º
            - model: –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –≤–µ—Å–∞–º–∏
            - device: –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    """
    # --- üå± –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å ---
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # --- üñ•Ô∏è Device setup ---
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    # --- ‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ª–æ—Å—Å ---
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    # --- üìà –õ–æ–≥–∏ ---
    train_losses = []
    val_accuracies = []
    best_val_acc = 0.0
    best_model_wts = None
    weight_model_new = None
    bias_model_new = None

    # --- üîÅ –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ---
    for epoch in range(epochs):
        # --- üü† –û–±—É—á–µ–Ω–∏–µ ---
        model.train()
        running_loss = 0.0
        progress_bar = tqdm(
            train_loader,
            desc=f"Epoch {epoch+1}/{epochs}",
            unit="batch",
            disable=not verbose,
            leave=False
        )

        for batch in progress_bar:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤: (img, label, extra) –∏–ª–∏ (img, label)
            if len(batch) == 3:
                images, labels, extra = batch
            else:
                images, labels = batch
                extra = None

            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ extra
            if extra is not None:
                extra = extra.to(device, non_blocking=True)

            optimizer.zero_grad()
            if extra is not None:
                outputs = model(images, extra)
            else:
                outputs = model(images)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")
        epoch_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_loss)

        # --- üü¢ –í–∞–ª–∏–¥–∞—Ü–∏—è ---
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch in val_loader:
                if len(batch) == 3:
                    images, labels, extra = batch
                else:
                    images, labels = batch
                    extra = None

                images = images.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)

                if extra is not None:
                    extra = extra.to(device, non_blocking=True)
                    outputs = model(images, extra)
                else:
                    outputs = model(images)

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        val_acc = correct / total
        val_accuracies.append(val_acc)
        # --- üèÜ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ ---
        if val_acc >= best_val_acc:
            best_val_acc = val_acc
            best_model_wts = model.state_dict().copy()

            last_conv = model.blocks[-1][0]
            weight_model_new = last_conv.weight  # [C_out, C_in, k, k]
            bias_model_new = last_conv.bias if last_conv.bias is not None else None

            if save_best_path:
                torch.save(best_model_wts, save_best_path)
                if verbose:
                    print(f"üî• Best model updated: val_acc = {val_acc:.4f}")

        # --- üì¢ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
        if verbose:
            print(f"Epoch [{epoch+1:2d}/{epochs}] | Loss: {epoch_loss:6.4f} | Val Acc: {val_acc:6.4f} | Best: {best_val_acc:6.4f}")

    # --- ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å ---
    if best_model_wts is not None:
        model.load_state_dict(best_model_wts)

    # --- üì¶ –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ---
    return {
        "best_val_acc": best_val_acc,
        "train_losses": train_losses,
        "val_accuracies": val_accuracies,
        "model": model,
        "new_weight": weight_model_new,
        "new_bias": bias_model_new,  # –î–æ–±–∞–≤–ª–µ–Ω–æ
        "device": device,
    }
