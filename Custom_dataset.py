# def progressive_architecture_search(
#     train_loader: DataLoader,
#     val_loader: DataLoader,
#     max_layers: int = 10,
#     k_candidates: List[int] = None,
#     epochs_per_eval: int = 5,
#     lr: float = 1e-3,
#     device: str = None,
#     seed: int = 42,
#     copy_weights:bool = True
#
# ) -> Tuple[ProgressiveModel, List[dict]]:
#     """
#     –ü–æ—à–∞–≥–æ–≤–æ –Ω–∞—Ä–∞—â–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å, –≤—ã–±–∏—Ä–∞—è –Ω–∞–∏–ª—É—á—à–∏–π –±–ª–æ–∫ (–ø–æ val_acc) –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ.
#
#     Args:
#         train_loader, val_loader: –¥–∞–Ω–Ω—ã–µ
#         max_layers: –º–∞–∫—Å. —á–∏—Å–ª–æ –±–ª–æ–∫–æ–≤
#         k_candidates: —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π k (–∫–∞–Ω–∞–ª—ã = base_channels * k)
#         epochs_per_eval: —Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö —É—á–∏—Ç—å –∫–∞–∂–¥—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
#         lr: learning rate
#         device: cuda/cpu
#         seed: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
#
#     Returns:
#         –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏ –∏—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞
#     """
#     if k_candidates is None:
#         k_candidates = [1, 3, 9, 27]  # –Ω–∞–ø—Ä–∏–º–µ—Ä
#
#     if device is None:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#
#     # üå± –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#
#     # üß± –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –º–æ–¥–µ–ª–∏
#     model = ProgressiveModel(extra_input_dim=0,class_data=2).to(device)
#     base_channels = 3  # –¥–ª—è RGB
#     best_val_acc = 0.0
#     history = []
#
#     print("üîç –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...")
#
#     for layer_idx in range(max_layers):
#         print(f"\n--- [–®–∞–≥ {layer_idx + 1}] –ü–æ–¥–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–ª–æ–∫–∞ ---")
#         best_k = None
#         best_c = None
#         best_acc_for_layer = 0.0
#         best_candidate_state = None
#         weight_model = None
#         bias_model = None
#
#         prev_out_channels = 3 if len(model.blocks) == 0 else model.blocks[-1][0].out_channels
#
#         # üí° –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ k
#         for k in k_candidates:
#
#             channels = base_channels * k
#             print(f"  –û—Ü–µ–Ω–∫–∞: –±–ª–æ–∫ ‚Üí {prev_out_channels} ‚Üí {channels} –∫–∞–Ω–∞–ª–æ–≤ (k={k})")
#
#             # üîÅ –ö–æ–ø–∏—Ä—É–µ–º –¢–ï–ö–£–©–£–Æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
#             candidate = ProgressiveModel(extra_input_dim=0,class_data=2)
#             # candidate = torch.compile(candidate, mode="max-autotune")
#             # –í–æ—Å—Å–æ–∑–¥–∞—ë–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–ª–æ–∫–∏
#             for block in model.blocks:
#                 in_ch = block[0].in_channels
#                 out_ch = block[0].out_channels
#                 candidate.add_block(in_ch, out_ch)
#             # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
#             candidate.add_block(prev_out_channels, channels)
#             print(f'–≠—Ç–æ –∫–∞–Ω–¥–∏–¥–∞—Ç {candidate}')
#             print(f'–≠—Ç–æ –º–æ–¥–µ–ª—å {model.blocks.weight}')
#             if weight_model is not None and bias_model is not None:
#                 with torch.no_grad():
#                     candidate.blocks[-1][0].weight.data.copy_(weight_model)
#                     if candidate.blocks[-1][0].bias is not None:
#                         candidate.blocks[-1][0].bias.data.copy_(bias_model)
#             candidate.to(device)
#
#             # üî¨ –û—Ü–µ–Ω–∏–≤–∞–µ–º (—Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏!)
#             result = train_and_evaluate_model(
#                 model=candidate,
#                 train_loader=train_loader,
#                 val_loader=val_loader,
#                 epochs=epochs_per_eval,
#                 lr=lr,
#                 weight_model=weight_model,
#                 device=device,
#                 save_best_path=None,  # –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
#                 verbose=True,
#             )
#             acc = result["best_val_acc"]
#             if copy_weights:
#                 new_weight = result["new_weight"]
#                 new_bias = result["new_bias"]
#                 weight_model = new_weight.detach().repeat(3, 1, 1, 1)  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ detach()
#                 bias_model = new_bias.detach().repeat(3) if new_bias is not None else None
#                 # üî• –î–û–ë–ê–í–õ–Ø–ï–ú –°–õ–£–ß–ê–ô–ù–û–°–¢–¨: weight noise (jitter)
#                 # –®—É–º ~ N(0, œÉ), –≥–¥–µ œÉ = 0.1 * std(–≤–µ—Å–æ–≤)
#                 noise_std = 0.03 * weight_model.std()
#                 weight_model += torch.randn_like(weight_model) * noise_std
#
#                 # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è bias (–µ—Å–ª–∏ –µ—Å—Ç—å)
#                 if bias_model is not None:
#                     bias_noise_std = 0.1 * bias_model.std()
#                     bias_model += torch.randn_like(bias_model) * bias_noise_std
#             else:
#                 weight_model = None
#                 bias_model = None
#             print(f"    ‚Üí val_acc = {acc:.4f}")
#             if acc > best_acc_for_layer:
#                 best_acc_for_layer = acc
#                 best_k = k
#                 best_c = channels
#                 best_candidate_state = {k: v.detach().clone() for k, v in candidate.state_dict().items()} # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞
#         # üèÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–æ–∏—Ç –ª–∏ —Ä–∞—Å—Ç–∏
#         weight_model = None
#         if best_acc_for_layer > best_val_acc:
#             # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–π –±–ª–æ–∫ –≤ –û–°–ù–û–í–ù–£–Æ –º–æ–¥–µ–ª—å
#             model.add_block(prev_out_channels, best_c)
#             model.load_state_dict(best_candidate_state)  # ‚Üê –ø–µ—Ä–µ–Ω–æ—Å–∏–º –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞!
#             best_val_acc = best_acc_for_layer
#
#             history.append({
#                 "layer": layer_idx + 1,
#                 "k": best_k,
#                 "channels": best_c,
#                 "val_acc": best_val_acc
#             })
#
#             print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –±–ª–æ–∫ {layer_idx + 1}: k={best_k} ‚Üí {best_c} –∫–∞–Ω–∞–ª–æ–≤, acc={best_val_acc:.4f}")
#         else:
#             print("‚ùå –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è ‚Äî –æ—Å—Ç–∞–Ω–æ–≤.")
#             break
#
#     print(f"\n‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {len(model.blocks)} –±–ª–æ–∫–æ–≤, acc={best_val_acc:.4f}")
#     return model, history
import random
ver = 50
reg = random.uniform(-ver,ver)
print(reg)
